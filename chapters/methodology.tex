\chapter{Methodology}

\section{CPU Benchmarking}
To generate CPU stress, we used sysbench \cite{sysbench}, which is a powerful cross-architecture tool, 
that can be used for Linux performance benchmarks. We used sysbench as a 
CPU stressing tool. When running with the CPU option, it performs the deterministic task of checking 
all prime numbers until reaching 10000 (default value) by doing standard division of the current number 
by all numbers between 2 and its square root \cite{gentoo_sysbench}. The number of the worker threads 
can be specified as an argument. The tool allows the specification of the aggregated number of events that 
should be performed by the created threads. We then use the total execution runtime as a comparison metric 
between the different experiments. For comparison purposes, we also developed our own CPU stressing 
tool called \textit{cpu\_burn} written in the C language. The program takes two arguments, the first 
being the number of operations that each created thread will perform and the second representing 
the number of worker threads. It then returns the total wallclock runtime that was needed for the 
execution of this job. 
We compiled the program using GCC with optimization level -O0, to ensure that no compilier optimizations
altered the program's behavior. Each thread executes the function defined under \texttt{perform\_work()}. 
The argument \texttt{work->operations} is specified by the user, as stated previously.

\begin{figure}[H]
\begin{lstlisting}[caption={Workload of the cpu\_burn tool}]
void* perform_work(void* arg) {
    ThreadWork* work = (ThreadWork*)arg;
    double x = 0.0;
    for (long long i = 0; i < work->operations; ++i) {
        x += i * 0.000001;
    }
    work->result = x;
    return NULL;
}
\end{lstlisting}
\end{figure}

\section{Network I/O Benchmarking}
\subsection{Throughput}
For network I/O stress, we used iPerf3 \cite{iperf}. It provides a benchmark for measuring network 
bandwidth. It supports various protocols and can be used to test TCP, UDP, and SCTP 
throughput.  The tool probes the maximum achievable network bandwidth by transmitting a large number 
of packets until reaching the thoughput's upper bound. In our experiments, we measured the 
maximum UDP throughput between clients and servers. We made this choice in order to avoid congestion 
effects that can be caused by TCP congestion control, which could reduce the throughput even though 
there still might be bandwidth available. 
\subsection{Latency}
For latency benchmarking, we used the sockperf tool \cite{sockperf}, which is a network benchmarking 
utility that can measure the latency of packets at a sub-nanosecond resolution. 
This tool introduces very low overhead as it uses Time Stamp Counter (TSC) registers that count 
the number of CPU cycles for measuring latency. We ran the command with the ping-pong argument on the 
client side and with the server argument on the server side.

\section{Testing Infrastructure}
All tests were performed in the AWS us-east-1 Ohio region. The EC2 instances ran Ubuntu Server 24.04 LTS
Linux. For each experiment, all the VMs were provisioned in the same availability zone and resided
in the same Virtual Private Cloud. This is particularly important for network I/O experiments, as 
the network traffic between VMs sharing the same AZ and VPC is free of charge. We ran parallel 
benchmarks on general-purpose and compute-intensive dedicated hosts from the 5th, 6th 
and 7th generations. We used mutliple instance types varying from large to 8xlarge instances.
To deploy the resources for the different experiments, we used Terraform which is an \ac{IaC} tool 
that's developed by HashiCorp and can be used to define and provision resources using the \ac{HCL}, 
ensuring the automation and reproducibility of the benchmarks. 
Additionally, we used distexprunner \cite{distex}, which is a tool written in python 
that helps write and run bash commands remotely across multiple nodes addressing them through 
their public IPs. Our experiments generated JSON or csv files that were gathered in an S3 
bucket using distexprunner. We also implemented multiple scripts in Python3 using mainly the re 
package \cite{re} for parsing raw data and csv \cite{csv} for working with comma separated data.
