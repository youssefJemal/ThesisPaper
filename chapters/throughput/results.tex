\subsection{m5 Family}
This subsection analyzes throughput contention on instances belonging to the m5 family. KPIs 
for the m5 dedicated host can be found in Table \ref{tab:dedicated-hosts}.
Table \ref{tab:m5_spec} depicts the most important network-related specifications for the different 
instance types.
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Type} & \textbf{vCPUs} & \textbf{Burst BW (Gbps) \cite{cloudspecs}} & \textbf{Baseline BW \cite{cloudspecs}} \\
\midrule
m5.large     & 2  & 10 & 0.75 \\
m5.xlarge    & 4  & 10 & 1.25 \\
m5.2xlarge   & 8  & 10 & 2.5  \\
m5.4xlarge   & 16 & 10 & 5    \\
m5.8xlarge   & 36 & 10 & 10   \\
m5.12xlarge  & 72 & 12 & 12   \\
m5.metal     & 96 & 25 & 25   \\
\bottomrule
\end{tabular}
\caption{Specifications of m5 instance types (BW = Bandwidth)}
\label{tab:m5_spec}
\end{table}
\noindent
The first experiment features the m5.4xlarge instance type. 
The maximum number of VMs on the host is 6.
To provide a clearer visualization of the results, we present each node in a separate graph in 
Figure \ref{fig:eachalone}.
\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{groupplot}[
    group style={
        group size= 2 by 4,
        horizontal sep=1.25cm,
        vertical sep=1.25cm,
        xlabels at= edge bottom,
        ylabels at=edge left,
    },
    width=6cm,
    height=4cm,
    xlabel={Number of Nodes},
    %ylabel={Throughput (Gbps)},
    title style={yshift=-1ex},
    ymin=2, ymax=10,
    xtick distance = 1, 
    xmin=0,
    grid=both,
]

% Node 1
\nextgroupplot[title={Node 1}]
\addplot[mark=*, blue, thick] coordinates {(0,9.96)(1,9.96) (2,8.06)(3,5.97) (4,4.90) (5, 4.01)};

% Node 2
\nextgroupplot[title={Node 2},]
\addplot[mark=square*, red, thick] coordinates {(1,9.96) (2,7.83)(3,5.97) (4,4.98) (5, 4.17)};
% Node 3
\nextgroupplot[title={Node 3}, ylabel = {Throughput (Gbps)}]
\addplot[mark=triangle*, green, thick] coordinates {(2,8.00) (3,5.98) (4,4.91) (5, 4.17)};


\nextgroupplot[title={Node 4}]
\addplot[mark=diamond*, orange, thick] coordinates {(3,5.96) (4, 4.98) (5,3.99)};

% Node 4
\nextgroupplot[title={Node 5}, xlabel={Number of Nodes}]
\addplot[mark=diamond*, orange, thick] coordinates {(4,4.91) (5, 4.11)};

% Node 4
\nextgroupplot[title={Node 6}, xlabel={Number of Nodes}]
\addplot[mark=diamond*, orange, thick] coordinates {(5,4.25)};
\end{groupplot}
\end{tikzpicture}
\caption{UDP Throughput of m5.4xlarge nodes when incrementally increasing the busy tenants}
\label{fig:eachalone}
\end{figure}

\noindent
The first and the second clients had access to a burst 
bandwidth of 9.96 Gbps. The addition of the third tenant caused the average throughput to drop to 
7.96 Gbps, while the fourth neighbor further reduced it to 5.96 Gbps. 
The fifth neighbor decreased the throughput of all co-tenants to roughly 
the baseline bandwidth of the m5.4xlarge (5 Gbps) with an average throughput of 4.94 Gbps. 
The sixth node introduced the first significant decrease below the baseline bandwidth to an average 
of 4.12 Gbps, which is 17.6\% less than the baseline. 
Starting from the third tenant, the sum of the throughputs across all nodes remained around 24.7 Gbps.  
This was expected as the bandwidth of the m5.metal is 25 Gbps, representing the upper limit for the 
sum of the throughputs of all the nodes residing on the same dedicated host. \\
For the xlarge, 2xlarge, and 4xlarge types, the product of the 
maximum number of nodes on the dedicated host multiplied by the baseline bandwidth of the respective
instance type equals 30 Gbps. This value is 16.7\% smaller than the 25 Gbps bandwidth 
of the physical server, which explains the average degradation of 17\% we observed in the previous 
experiment. The same behavior should be expected when using xlarge and 2xlarge instances. 
For the large instance type, however, the product is equal to 
36 Gbps (\begin{math} 48 \times 0.75\end{math}).
25 Gbps is 30\% smaller than 36 Gbps, indicating that a  
degradation of around 30\% is anticipated at full capacity when using m5.large instances. 
We verify this assumption in the next experiment. Since the dedicated host can host 48 of m5.large 
instances, we cannot plot the graph of each instance. Instead, we used a plotbox graph to display the 
distribution of the throughputs at different occupancy levels, as shown in figure \ref{fig::boxplot}.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    boxplot/draw direction=y,
    ylabel={Throughput},
    xlabel={Number of Nodes},
    xtick={1,2,3,4,5,6, 7, 8},
    xticklabels={8, 16, 24, 32, 40, 48},
    ymin = 0.2,
    ymax = 4,
    width=11cm,
    height=7cm,
    grid = both,
]


%\addplot+[
%    boxplot,
%    draw=black,
%] table[row sep=\\, y index=0] {
%data \\
%9.96 \\ 
%}; 

% Boxplot A
\addplot+[
    boxplot,
    draw=black,
] table[row sep=\\, y index=0] {
data \\
2.19 \\ 2.17 \\ 2.19 \\ 3.19 \\ 3.88 \\ 3.17 \\ 3.87 \\ 3.21 \\
};

% Boxplot B
\addplot+[
    boxplot,
    draw=black,
] table[row sep=\\, y index=0] {
data \\
1.12 \\ 1.12 \\ 1.12 \\ 1.82 \\ 2.03 \\ 1.82 \\ 2.00 \\ 1.80 \\ 
1.12 \\ 1.12 \\ 1.18 \\ 2.13 \\ 1.18 \\ 1.19 \\ 1.12 \\ 1.97 \\
};

% Boxplot C
\addplot+[
    boxplot,
    draw=black,
] table[row sep=\\, y index=0] {
data \\
0.86 \\ 0.86 \\ 0.86 \\ 1.16 \\ 1.21 \\ 1.16 \\ 1.20 \\ 1.14 \\ 
0.86 \\ 0.86 \\ 0.86 \\ 1.25 \\ 0.86 \\ 0.85 \\ 0.86 \\ 1.18 \\ 
0.85 \\ 0.85 \\ 1.14 \\ 0.86 \\ 1.13 \\ 0.86 \\ 0.86 \\ 1.27 \\
};

% Boxplot D
\addplot+[
    boxplot,
    draw=black,
] table[row sep=\\, y index=0] {
data \\
0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 
0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 
0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 
0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\ 0.75 \\
};
% Boxplot E
\addplot+[
    boxplot,
    draw=black,
] table[row sep=\\, y index=0] {
data \\
0.28 \\ 0.28 \\ 0.27 \\ 1.15 \\ 0.84 \\ 0.85 \\ 0.84 \\ 1.10 \\ 
0.31 \\ 0.29 \\ 0.21 \\ 1.14 \\ 0.21 \\ 0.19 \\ 0.31 \\ 0.87 \\ 
1.11 \\ 1.09 \\ 1.22 \\ 1.26 \\ 1.20 \\ 0.29 \\ 1.59 \\ 1.46 \\ 
0.23 \\ 0.20 \\ 0.55 \\ 0.53 \\ 0.44 \\ 0.54 \\ 0.26 \\ 0.52 \\ 
0.26 \\ 0.23 \\ 0.82 \\ 0.78 \\ 0.21 \\ 0.20 \\ 0.22 \\ 0.21 \\
};
% Boxplot F
\addplot+[
    boxplot,
    draw=black,
    solid,
] table[row sep=\\, y index=0] {
data \\
0.70 \\ 0.67 \\ 0.14 \\ 0.98 \\ 0.76 \\ 0.74 \\ 0.59 \\ 0.94 \\ 
0.13 \\ 0.71 \\ 0.28 \\ 0.58 \\ 0.18 \\ 0.17 \\ 0.13 \\ 0.60 \\ 
0.16 \\ 0.19 \\ 0.96 \\ 0.20 \\ 0.97 \\ 0.15 \\ 0.70 \\ 0.89 \\ 
0.19 \\ 0.18 \\ 0.76 \\ 0.89 \\ 0.97 \\ 0.88 \\ 0.72 \\ 0.76 \\ 
0.14 \\ 0.75 \\ 0.62 \\ 0.20 \\ 0.20 \\ 0.18 \\ 0.23 \\ 0.18 \\ 
0.16 \\ 0.57 \\ 0.58 \\ 0.92 \\ 0.75 \\ 0.56 \\ 0.15 \\ 0.58 \\
};
\end{axis}
\end{tikzpicture}
\caption{Throughput (UDP) of m5.large nodes when incrementally increasing the number of busy tenants}
\label{fig::boxplot}
\end{figure}
\noindent
The first node has access to a burst bandwidth of 9.96 Gbps.
At eight neighbors, the average throughput dropped to 3 Gbps. We then observe a gradual degradation 
with an average of 1.5 Gbps at 16 nodes, 1 Gbps at 24 nodes, and  0.75 Gbps at 32 nodes,  which 
corresponds to the baseline bandwidth of the m5.large instance type. 
At this level, we observed virtually no variation between the throughput levels. 
At 40 nodes, the average bandwidth decreased to 0.614 Gbps and further to 0.51 Gbps at 48 nodes.  
At full capacity, The average throughput (0.51 Gbps) was 30.7\% lower than the baseline bandwidth of the 
m5.large instance (0.75 Gbps). This more pronounced performance degradation close to 30\%, aligned
with our earlier hypothesis. In this experiment, we also observed a notable performance variation 
between the different nodes compared to the previous experiment.
