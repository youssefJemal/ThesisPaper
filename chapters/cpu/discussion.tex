\section{Discussion}
The m5 instances are prone to CPU contention, with performance degradation
ranging from 3.25\% (4xlarge) to 13\% (large) (Table \ref{tab::all_m5}). 
For this family in particular, experiments show that degradation caused by adding idle neighbors is on par
with the degradation from adding busy neighbors. Han et al. \cite{characterizing_public} 
observed this behavior and were able to improve sysbench performance by 20.81\% 
on an m5d.large instance by shutting down idle VMs. This behavior seems unique to the m5 family as 
they could not replicate it on the c3 and c4 families. \\ 
The m6i family showed only minor degradation with values around 2\%. In contrast, the m6a instances 
showed more important degradation with values reaching 11\%. In both families, 
idle instances practically did not affect the test node's performance. \\
Han et al. \cite{characterizing_public} suggested that context switching overhead introduced
by the Nitro scheduler could create performance degradation.
However, this explanation is implausible, since the Nitro System provides a near bare-metal 
performance as seen for the m6g family (see Figure \ref{fig::m6g_metal_vs_VMs}). 
Thus, our first hypothesis is accepted. 
The second hypothesis is rejected, since we found that instances with $n$ vCPUs running on an SMT-enabled host only have 
access to $n/2$ physical cores, which excludes any possible degradation caused by physical core 
co-location between different co-tenants. This distribution, however,  affects the initial performance 
of deployed VMs, as they cannot benefit from non-occupied cores. This is reflected in the initial 
difference between the runtime of the first deployed instance on the dedicated host and the runtime 
of running the threads directly on the metal instance. This difference reached 36\% on m5, 27\% on m6i, 
and 9.3\% on m6a using large instances. \\
We argue that the observed performance degradation when adding busy neighbors is caused 
by the frequency scaling feature. We confirmed this in our experiment with the m7a family, 
where we measured a decrease in the average frequency of the busy cores as the number of busy 
threads are increase (see Figure \ref{tab::m7a}). This finding validates the third hypothesis. 
For the m6g dedicated host, we witnessed almost no CPU contention with degradation 
values lower than 0.05\%. This behvaior was expected, since Graviton processors do not support frequency
scaling, and the virtualization overhead is minimal.
